#!/usr/bin/env python3
"""
Step 8: Performance Optimization Demo

This script demonstrates:
- MinIO performance tuning and optimization
- Iceberg query performance optimization
- Network latency mitigation strategies  
- Caching patterns and implementations
- Connection pooling and retry strategies
- Performance monitoring and profiling
"""

import concurrent.futures
import os
import random
import sys
import time
from contextlib import contextmanager
from datetime import datetime, timedelta
from pathlib import Path
from statistics import mean, median

import boto3
import pandas as pd
from botocore.client import Config
from botocore.exceptions import ClientError
from dotenv import load_dotenv
from pyiceberg.catalog.sql import SqlCatalog
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn
from rich.table import Table

console = Console()

def load_environment():
    """Load environment configuration"""
    env_file = Path('.env')
    if env_file.exists():
        load_dotenv(env_file, override=True)
        console.print(f"‚úÖ Loaded environment from: [green]{env_file}[/green]")
        return True
    else:
        console.print(f"‚ùå Environment file not found: {env_file}")
        return False

def get_optimized_s3_client():
    """Get S3 client with optimized configuration"""
    endpoint = os.getenv('MINIO_ENDPOINT')
    access_key = os.getenv('MINIO_ACCESS_KEY')
    secret_key = os.getenv('MINIO_SECRET_KEY')
    region = os.getenv('MINIO_REGION', 'us-east-1')
    
    # Optimized client configuration
    config = Config(
        region_name=region,
        retries={
            'max_attempts': 3,
            'mode': 'adaptive'
        },
        max_pool_connections=50,  # Increased connection pool
        read_timeout=60,
        connect_timeout=10,
    )
    
    return boto3.client(
        's3',
        endpoint_url=endpoint,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        config=config,
        use_ssl=endpoint.startswith('https'),
        verify=True if endpoint.startswith('https') else False
    )

def get_basic_s3_client():
    """Get S3 client with basic configuration for comparison"""
    endpoint = os.getenv('MINIO_ENDPOINT')
    access_key = os.getenv('MINIO_ACCESS_KEY')
    secret_key = os.getenv('MINIO_SECRET_KEY')
    region = os.getenv('MINIO_REGION', 'us-east-1')
    
    return boto3.client(
        's3',
        endpoint_url=endpoint,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        region_name=region,
        use_ssl=endpoint.startswith('https'),
        verify=True if endpoint.startswith('https') else False
    )

def get_iceberg_catalog():
    """Get Iceberg catalog"""
    catalog_config = {
        "type": "sql",
        "uri": os.getenv('PYICEBERG_CATALOG__MINIO_LOCAL__URI', 'sqlite:///catalog.db'),
        "warehouse": os.getenv('PYICEBERG_CATALOG__MINIO_LOCAL__WAREHOUSE', 's3://iceberg-warehouse/'),
        "s3.endpoint": os.getenv('MINIO_ENDPOINT'),
        "s3.access-key-id": os.getenv('MINIO_ACCESS_KEY'),
        "s3.secret-access-key": os.getenv('MINIO_SECRET_KEY'),
        "s3.region": os.getenv('MINIO_REGION', 'us-east-1'),
        "s3.path-style-access": "true"
    }
    
    return SqlCatalog("minio_local", **catalog_config)

def display_performance_overview():
    """Display performance optimization overview"""
    perf_panel = Panel.fit(
        """[bold cyan]Performance Optimization Strategy[/bold cyan]

üéØ [bold]Key Performance Areas:[/bold]
‚Ä¢ [green]Network Latency[/green] - Minimize round trips to object storage
‚Ä¢ [yellow]Throughput[/yellow] - Maximize data transfer rates
‚Ä¢ [blue]Concurrency[/blue] - Parallel operations and connection pooling
‚Ä¢ [magenta]Caching[/magenta] - Reduce redundant metadata and data fetches

üöÄ [bold]MinIO Optimizations:[/bold]
‚Ä¢ Connection pooling and keep-alive
‚Ä¢ Multipart uploads for large files
‚Ä¢ Parallel operations with thread pools
‚Ä¢ Regional proximity and CDN usage

‚ö° [bold]Iceberg Optimizations:[/bold]  
‚Ä¢ Metadata caching and local catalogs
‚Ä¢ File size optimization (128MB-1GB per file)
‚Ä¢ Partition pruning and column projection
‚Ä¢ Snapshot expiration and compaction""",
        title="Performance Optimization",
        border_style="yellow"
    )
    console.print(perf_panel)

@contextmanager
def performance_timer(operation_name):
    """Context manager to time operations"""
    start_time = time.time()
    yield
    end_time = time.time()
    duration = end_time - start_time
    console.print(f"‚è±Ô∏è  {operation_name}: [yellow]{duration:.3f} seconds[/yellow]")

def benchmark_connection_configurations():
    """Benchmark different S3 client configurations"""
    console.print("\n" + "="*60)
    console.print("üìä CONNECTION CONFIGURATION BENCHMARK")
    console.print("="*60)
    
    bucket_name = "iceberg-warehouse"
    
    # Test configurations
    configs = [
        {
            "name": "Basic Configuration",
            "client": get_basic_s3_client(),
            "description": "Default boto3 client settings"
        },
        {
            "name": "Optimized Configuration", 
            "client": get_optimized_s3_client(),
            "description": "Tuned for performance with connection pooling"
        }
    ]
    
    # Benchmark each configuration
    results = []
    
    for config in configs:
        console.print(f"\nüß™ Testing: [cyan]{config['name']}[/cyan]")
        console.print(f"   {config['description']}")
        
        client = config['client']
        
        # Test 1: List operations
        list_times = []
        for i in range(5):
            with performance_timer(f"List operation {i+1}"):
                start = time.time()
                try:
                    response = client.list_objects_v2(Bucket=bucket_name, MaxKeys=100)
                    list_times.append(time.time() - start)
                except Exception as e:
                    console.print(f"‚ö†Ô∏è  List operation failed: {e}")
                    list_times.append(999)  # High penalty for failure
        
        # Test 2: Head operations (metadata only)
        head_times = []
        try:
            # First, get some objects to test
            response = client.list_objects_v2(Bucket=bucket_name, MaxKeys=10)
            objects = response.get('Contents', [])
            
            if objects:
                for obj in objects[:3]:  # Test first 3 objects
                    start = time.time()
                    try:
                        client.head_object(Bucket=bucket_name, Key=obj['Key'])
                        head_times.append(time.time() - start)
                    except Exception as e:
                        head_times.append(999)
            else:
                # Create a test object if none exist
                test_key = f"perf-test/test-{int(time.time())}.txt"
                client.put_object(Bucket=bucket_name, Key=test_key, Body="test data")
                
                start = time.time()
                client.head_object(Bucket=bucket_name, Key=test_key)
                head_times.append(time.time() - start)
        
        except Exception as e:
            console.print(f"‚ö†Ô∏è  Head operation test failed: {e}")
            head_times = [999]
        
        results.append({
            "name": config["name"],
            "avg_list_time": mean(list_times) if list_times else 999,
            "avg_head_time": mean(head_times) if head_times else 999,
            "list_times": list_times,
            "head_times": head_times
        })
    
    # Display results
    perf_table = Table()
    perf_table.add_column("Configuration", style="cyan")
    perf_table.add_column("Avg List Time", style="green")
    perf_table.add_column("Avg Head Time", style="yellow")
    perf_table.add_column("Performance Improvement", style="blue")
    
    baseline_list = results[0]["avg_list_time"]
    baseline_head = results[0]["avg_head_time"]
    
    for result in results:
        list_improvement = ((baseline_list - result["avg_list_time"]) / baseline_list * 100) if baseline_list > 0 else 0
        head_improvement = ((baseline_head - result["avg_head_time"]) / baseline_head * 100) if baseline_head > 0 else 0
        
        avg_improvement = (list_improvement + head_improvement) / 2
        
        improvement_text = f"{avg_improvement:+.1f}%" if avg_improvement != 0 else "baseline"
        
        perf_table.add_row(
            result["name"],
            f"{result['avg_list_time']:.3f}s",
            f"{result['avg_head_time']:.3f}s", 
            improvement_text
        )
    
    console.print(perf_table)

def demonstrate_parallel_operations():
    """Demonstrate parallel vs sequential operations"""
    console.print("\n" + "="*60)
    console.print("‚ö° PARALLEL OPERATIONS DEMO")
    console.print("="*60)
    
    client = get_optimized_s3_client()
    bucket_name = "iceberg-warehouse"
    
    # Create test data
    test_keys = []
    test_data = "x" * 1024  # 1KB test data
    
    console.print("üîß Creating test objects...")
    for i in range(10):
        key = f"perf-test/parallel-test-{i:03d}.txt"
        try:
            client.put_object(Bucket=bucket_name, Key=key, Body=test_data)
            test_keys.append(key)
        except Exception as e:
            console.print(f"‚ö†Ô∏è  Failed to create {key}: {e}")
    
    if not test_keys:
        console.print("‚ùå No test objects created, skipping parallel demo")
        return
    
    console.print(f"‚úÖ Created {len(test_keys)} test objects")
    
    # Sequential operations benchmark
    console.print("\nüìä Sequential operations:")
    with performance_timer("Sequential download"):
        sequential_start = time.time()
        sequential_sizes = []
        for key in test_keys:
            try:
                response = client.get_object(Bucket=bucket_name, Key=key)
                data = response['Body'].read()
                sequential_sizes.append(len(data))
            except Exception as e:
                console.print(f"‚ö†Ô∏è  Failed to download {key}: {e}")
        sequential_time = time.time() - sequential_start
    
    # Parallel operations benchmark
    console.print("üìä Parallel operations:")
    
    def download_object(key):
        try:
            response = client.get_object(Bucket=bucket_name, Key=key)
            data = response['Body'].read()
            return len(data)
        except Exception:
            return 0
    
    with performance_timer("Parallel download"):
        parallel_start = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            parallel_sizes = list(executor.map(download_object, test_keys))
        parallel_time = time.time() - parallel_start
    
    # Compare results
    comparison_table = Table()
    comparison_table.add_column("Approach", style="cyan")
    comparison_table.add_column("Time", style="green")
    comparison_table.add_column("Objects", style="yellow")
    comparison_table.add_column("Throughput", style="blue")
    comparison_table.add_column("Speedup", style="magenta")
    
    total_bytes = sum(sequential_sizes)
    sequential_throughput = total_bytes / sequential_time if sequential_time > 0 else 0
    parallel_throughput = sum(parallel_sizes) / parallel_time if parallel_time > 0 else 0
    speedup = sequential_time / parallel_time if parallel_time > 0 else 1
    
    comparison_table.add_row(
        "Sequential",
        f"{sequential_time:.3f}s",
        str(len(sequential_sizes)),
        f"{sequential_throughput/1024:.1f} KB/s",
        "1.0x (baseline)"
    )
    
    comparison_table.add_row(
        "Parallel (5 threads)",
        f"{parallel_time:.3f}s", 
        str(len(parallel_sizes)),
        f"{parallel_throughput/1024:.1f} KB/s",
        f"{speedup:.1f}x faster"
    )
    
    console.print(comparison_table)
    
    # Cleanup test objects
    console.print("\nüßπ Cleaning up test objects...")
    for key in test_keys:
        try:
            client.delete_object(Bucket=bucket_name, Key=key)
        except Exception:
            pass

def demonstrate_caching_patterns():
    """Demonstrate metadata caching patterns"""
    console.print("\n" + "="*60)
    console.print("üíæ METADATA CACHING PATTERNS")
    console.print("="*60)
    
    caching_panel = Panel.fit(
        """[bold cyan]Caching Strategy for Object Storage[/bold cyan]

üéØ [bold]What to Cache:[/bold]
‚Ä¢ [green]Catalog Metadata[/green] - Table schemas, partition info
‚Ä¢ [yellow]File Listings[/yellow] - Directory contents and file metadata
‚Ä¢ [blue]Statistics[/blue] - Row counts, data distributions, file sizes  
‚Ä¢ [magenta]Query Plans[/magenta] - Optimized execution plans

‚ö° [bold]Caching Levels:[/bold]
‚Ä¢ [green]Application Level[/green] - In-memory caches (Redis, Memcached)
‚Ä¢ [yellow]Client Level[/yellow] - Local file system cache
‚Ä¢ [blue]Network Level[/blue] - CDN and edge caching
‚Ä¢ [magenta]Storage Level[/magenta] - MinIO built-in caching

üîÑ [bold]Cache Invalidation:[/bold]
‚Ä¢ Time-based expiration (TTL)
‚Ä¢ Event-driven invalidation
‚Ä¢ Version-based invalidation
‚Ä¢ Manual cache refresh""",
        title="Caching Patterns",
        border_style="green"
    )
    console.print(caching_panel)
    
    # Simulate cache performance comparison
    console.print("üìä Cache performance simulation:")
    
    # Simulate metadata operations with/without cache
    cache_scenarios = [
        {
            "scenario": "Cold Cache (No Cache)",
            "metadata_lookup": 0.150,  # 150ms per lookup
            "file_listing": 0.200,     # 200ms per listing
            "operations": 100
        },
        {
            "scenario": "Warm Cache (Local)",
            "metadata_lookup": 0.005,  # 5ms per lookup (cached)
            "file_listing": 0.010,     # 10ms per listing (cached)
            "operations": 100
        },
        {
            "scenario": "Distributed Cache (Redis)",
            "metadata_lookup": 0.020,  # 20ms per lookup (network cache)
            "file_listing": 0.030,     # 30ms per listing (network cache)
            "operations": 100
        }
    ]
    
    cache_table = Table()
    cache_table.add_column("Cache Type", style="cyan")
    cache_table.add_column("Metadata Time", style="green")
    cache_table.add_column("File List Time", style="yellow")
    cache_table.add_column("Total Time", style="blue")
    cache_table.add_column("Performance", style="magenta")
    
    baseline_time = None
    
    for scenario in cache_scenarios:
        metadata_total = scenario["metadata_lookup"] * scenario["operations"]
        listing_total = scenario["file_listing"] * scenario["operations"]
        total_time = metadata_total + listing_total
        
        if baseline_time is None:
            baseline_time = total_time
            performance = "Baseline"
        else:
            improvement = (baseline_time - total_time) / baseline_time * 100
            performance = f"{improvement:.0f}% faster"
        
        cache_table.add_row(
            scenario["scenario"],
            f"{metadata_total:.1f}s",
            f"{listing_total:.1f}s",
            f"{total_time:.1f}s",
            performance
        )
    
    console.print(cache_table)

def demonstrate_query_optimization():
    """Demonstrate Iceberg query optimization patterns"""
    console.print("\n" + "="*60)
    console.print("üîç ICEBERG QUERY OPTIMIZATION")
    console.print("="*60)
    
    try:
        catalog = get_iceberg_catalog()
        
        # Create a sample table for optimization demo
        table_name = "perf_demo.transactions"
        
        # Check if table exists, drop and recreate for clean demo
        try:
            if catalog.table_exists(table_name):
                catalog.drop_table(table_name)
        except Exception:
            pass
        
        # Create optimized table schema
        from pyiceberg.schema import Schema
        from pyiceberg.types import NestedField, StringType, DoubleType, DateType, LongType
        from pyiceberg.partitioning import PartitionSpec, PartitionField
        from pyiceberg.transforms import day
        
        schema = Schema(
            NestedField(1, "transaction_id", LongType(), required=True),
            NestedField(2, "user_id", StringType(), required=True),
            NestedField(3, "amount", DoubleType(), required=True),
            NestedField(4, "transaction_date", DateType(), required=True),
            NestedField(5, "category", StringType(), required=True),
            NestedField(6, "region", StringType(), required=True),
        )
        
        # Partition by date for optimal performance
        partition_spec = PartitionSpec(
            PartitionField(source_id=4, field_id=1000, transform=day, name="date_day")
        )
        
        table = catalog.create_table(table_name, schema, partition_spec=partition_spec)
        console.print(f"‚úÖ Created optimized table: [green]{table_name}[/green]")
        console.print("   ‚Ä¢ Partitioned by transaction_date (daily partitions)")
        console.print("   ‚Ä¢ Optimized schema with appropriate data types")
        
        # Generate sample data across multiple days
        console.print("\nüìä Generating sample data...")
        
        base_date = datetime.now().date() - timedelta(days=30)
        data_batches = []
        
        for day_offset in range(30):  # 30 days of data
            current_date = base_date + timedelta(days=day_offset)
            
            # Generate transactions for this day
            daily_data = []
            for i in range(1000):  # 1000 transactions per day
                daily_data.append({
                    "transaction_id": day_offset * 1000 + i + 1,
                    "user_id": f"user_{(i % 100) + 1:03d}",
                    "amount": round(random.uniform(10.0, 1000.0), 2),
                    "transaction_date": current_date,
                    "category": random.choice(["food", "transport", "shopping", "entertainment", "utilities"]),
                    "region": random.choice(["north", "south", "east", "west", "central"])
                })
            
            df = pd.DataFrame(daily_data)
            table.append(df)
        
        console.print(f"‚úÖ Added 30,000 transactions across 30 days")
        
        # Demonstrate query optimization techniques
        optimization_examples = [
            {
                "technique": "Partition Pruning",
                "description": "Filter by partition column to skip irrelevant files",
                "query": "WHERE transaction_date >= '2024-01-15'",
                "benefit": "Scans only relevant date partitions"
            },
            {
                "technique": "Column Projection",
                "description": "Select only needed columns", 
                "query": "SELECT user_id, amount FROM table",
                "benefit": "Reduces data transfer by ~60%"
            },
            {
                "technique": "Predicate Pushdown",
                "description": "Apply filters at storage layer",
                "query": "WHERE amount > 500 AND category = 'shopping'",
                "benefit": "Filters data before network transfer"
            },
            {
                "technique": "File Size Optimization",
                "description": "Maintain optimal file sizes (128MB-1GB)",
                "query": "CALL optimize_table_files()",
                "benefit": "Balances parallelism and overhead"
            }
        ]
        
        opt_table = Table()
        opt_table.add_column("Optimization", style="cyan")
        opt_table.add_column("Description", style="white", width=25)
        opt_table.add_column("Example", style="green", width=25)
        opt_table.add_column("Performance Benefit", style="yellow", width=20)
        
        for example in optimization_examples:
            opt_table.add_row(
                example["technique"],
                example["description"],
                example["query"],
                example["benefit"]
            )
        
        console.print(opt_table)
        
        # Simulate query performance with different approaches
        console.print("\n‚ö° Query performance simulation:")
        
        query_scenarios = [
            {
                "approach": "Full Table Scan",
                "files_scanned": 30,
                "data_scanned": "30MB",
                "time_estimate": "2.5s"
            },
            {
                "approach": "With Partition Pruning",
                "files_scanned": 7,
                "data_scanned": "7MB", 
                "time_estimate": "0.6s"
            },
            {
                "approach": "With Column Projection",
                "files_scanned": 7,
                "data_scanned": "2.8MB",
                "time_estimate": "0.3s"
            },
            {
                "approach": "Fully Optimized",
                "files_scanned": 3,
                "data_scanned": "1.2MB",
                "time_estimate": "0.1s"
            }
        ]
        
        query_perf_table = Table()
        query_perf_table.add_column("Query Approach", style="cyan")
        query_perf_table.add_column("Files Scanned", style="green")
        query_perf_table.add_column("Data Scanned", style="yellow")
        query_perf_table.add_column("Est. Time", style="blue")
        query_perf_table.add_column("Speedup", style="magenta")
        
        baseline_time = 2.5
        for scenario in query_scenarios:
            time_val = float(scenario["time_estimate"].replace('s', ''))
            speedup = f"{baseline_time / time_val:.1f}x" if time_val > 0 else "‚àûx"
            
            query_perf_table.add_row(
                scenario["approach"],
                str(scenario["files_scanned"]),
                scenario["data_scanned"],
                scenario["time_estimate"],
                speedup
            )
        
        console.print(query_perf_table)
        
    except Exception as e:
        console.print(f"‚ùå Query optimization demo failed: {e}")

def demonstrate_monitoring_and_profiling():
    """Demonstrate performance monitoring and profiling"""
    console.print("\n" + "="*60)
    console.print("üìä PERFORMANCE MONITORING")
    console.print("="*60)
    
    monitoring_panel = Panel.fit(
        """[bold cyan]Performance Monitoring Strategy[/bold cyan]

üìà [bold]Key Metrics to Monitor:[/bold]
‚Ä¢ [green]Latency[/green] - P50, P95, P99 response times
‚Ä¢ [yellow]Throughput[/yellow] - Requests per second, bytes per second
‚Ä¢ [blue]Error Rates[/blue] - Failed requests and retry counts
‚Ä¢ [magenta]Resource Usage[/magenta] - CPU, memory, network utilization

üîß [bold]Monitoring Tools:[/bold]
‚Ä¢ [green]MinIO Built-in Metrics[/green] - Prometheus endpoint
‚Ä¢ [yellow]Application Metrics[/yellow] - Custom timing and counters
‚Ä¢ [blue]Infrastructure Metrics[/blue] - System and network monitoring
‚Ä¢ [magenta]Distributed Tracing[/magenta] - Request flow analysis

‚ö° [bold]Performance Alerts:[/bold]
‚Ä¢ High latency threshold breaches
‚Ä¢ Throughput degradation
‚Ä¢ Error rate spikes
‚Ä¢ Resource exhaustion warnings""",
        title="Performance Monitoring",
        border_style="blue"
    )
    console.print(monitoring_panel)
    
    # Sample performance metrics
    console.print("üìä Sample performance metrics:")
    
    metrics = [
        {
            "metric": "Average Response Time",
            "current": "45ms",
            "target": "< 50ms",
            "status": "‚úÖ Good"
        },
        {
            "metric": "P95 Response Time", 
            "current": "120ms",
            "target": "< 100ms",
            "status": "‚ö†Ô∏è  Warning"
        },
        {
            "metric": "Throughput",
            "current": "850 req/s",
            "target": "> 500 req/s", 
            "status": "‚úÖ Good"
        },
        {
            "metric": "Error Rate",
            "current": "0.2%",
            "target": "< 0.5%",
            "status": "‚úÖ Good"
        },
        {
            "metric": "Connection Pool Usage",
            "current": "75%",
            "target": "< 80%",
            "status": "‚úÖ Good"
        },
        {
            "metric": "Cache Hit Rate",
            "current": "92%",
            "target": "> 85%",
            "status": "‚úÖ Good"
        }
    ]
    
    metrics_table = Table()
    metrics_table.add_column("Metric", style="cyan")
    metrics_table.add_column("Current", style="white")
    metrics_table.add_column("Target", style="green")
    metrics_table.add_column("Status", style="bold")
    
    for metric in metrics:
        metrics_table.add_row(
            metric["metric"],
            metric["current"],
            metric["target"],
            metric["status"]
        )
    
    console.print(metrics_table)

def create_performance_checklist():
    """Create performance optimization checklist"""
    console.print("\n" + "="*60)
    console.print("‚úÖ PERFORMANCE OPTIMIZATION CHECKLIST")
    console.print("="*60)
    
    checklist_sections = [
        ("Client Configuration", [
            "Use connection pooling (max_pool_connections=50+)",
            "Enable retry policies with adaptive mode",
            "Set appropriate timeout values",
            "Use regional endpoints when possible"
        ]),
        ("Data Organization", [
            "Partition tables by frequently queried columns",
            "Maintain optimal file sizes (128MB-1GB)",
            "Regular compaction and file optimization",
            "Schema evolution planning"
        ]),
        ("Query Optimization", [
            "Use partition pruning in WHERE clauses",
            "Select only needed columns (projection)",
            "Leverage predicate pushdown",
            "Monitor and optimize query plans"
        ]),
        ("Caching Strategy", [
            "Implement metadata caching",
            "Cache frequently accessed data",
            "Use CDN for static content",
            "Monitor cache hit rates"
        ]),
        ("Monitoring", [
            "Set up performance monitoring dashboards",
            "Configure alerting for key metrics",
            "Regular performance testing",
            "Capacity planning and scaling"
        ])
    ]
    
    for section, items in checklist_sections:
        console.print(f"\n[bold cyan]{section}:[/bold cyan]")
        for item in items:
            console.print(f"  ‚ñ° {item}")

def main():
    """Main execution flow"""
    console.print("üöÄ MinIO Performance Optimization Demo")
    console.print("=" * 60)
    
    # Load environment
    if not load_environment():
        console.print("‚ùå Cannot proceed without environment configuration")
        return False
    
    # Display performance overview
    display_performance_overview()
    
    # Benchmark connection configurations
    benchmark_connection_configurations()
    
    # Demonstrate parallel operations
    demonstrate_parallel_operations()
    
    # Show caching patterns
    demonstrate_caching_patterns()
    
    # Query optimization
    demonstrate_query_optimization()
    
    # Performance monitoring
    demonstrate_monitoring_and_profiling()
    
    # Performance checklist
    create_performance_checklist()
    
    # Summary
    console.print("\n" + "="*60)
    console.print("üéØ PERFORMANCE OPTIMIZATION DEMO COMPLETE")
    console.print("="*60)
    
    console.print("‚úÖ [green]What we covered:[/green]")
    console.print("‚Ä¢ S3 client configuration optimization")
    console.print("‚Ä¢ Parallel vs sequential operation patterns")
    console.print("‚Ä¢ Metadata caching strategies") 
    console.print("‚Ä¢ Iceberg query optimization techniques")
    console.print("‚Ä¢ Performance monitoring and profiling")
    
    console.print("\nüìù [yellow]Key performance gains:[/yellow]")
    console.print("‚Ä¢ 2-5x faster with optimized client configuration")
    console.print("‚Ä¢ 3-10x speedup with parallel operations")
    console.print("‚Ä¢ 10-50x faster with effective caching")
    console.print("‚Ä¢ 5-25x query speedup with partitioning and projection")
    console.print("‚Ä¢ Real-time monitoring prevents performance degradation")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)